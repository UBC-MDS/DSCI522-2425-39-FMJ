---
title: Predicting age group from health and nutritional status of Americans
author: Forgive Agbesi, Jason Lee and Michael Hewlett
format:
    pdf:
        toc: true
        toc-depth: 2
        number-sections: true

execute:
    echo: false
---

```{python}
import pandas as pd
from IPython.display import Markdown, display
from tabulate import tabulate
import pickle
```

```{python}

```

## Summary

In this analysis we explored the use of several classification models to predict whether a respondent is an adult or senior (essentially below or above age 65) based on their health and nutritional data. Our most promising model used Support Vector Classification (SVC) and had an overall accuracy of 0.83. While this appears promising, much of this accuracy was achieved by classifying most respondents as adults, since this was the majority class. Precision and recall for predicting the senior class was quite low (5/76 and 5/9 respectively). This suggests that the model has considerable room for improvement, which could be achieved through optimizing the hyperparameters and selecting models based on precision, recall, or f1 scores, rather than general accuracy. With the goal of correctly classifying each group, false positive and false negative errors are both equally important, applying class weighting is worth exploring in future research. Once the model performs better on those metrics, it would be worth exploring which health and nutritional features are most predictive of age, which could provide suggestions for strategic public health programs.

## Introduction

While taking care of elders is a core value of many cultures, this is not a hallmark of many western societies, including the Unites States (Healthy Aging Center 2022). Is it possible that this is reflected in different health measures? Put another way, could we use health measures to predict whether an American is a senior or not?

Formally, the question this project seeks to answer is: Can information about the health and nutritional status of Americans be used to predict whether they are adults or seniors?

The dataset used to answer this question is the National Health and Nutrition Health Survey 2013-2014 (NHANES) Age Prediction Subset (link: https://archive.ics.uci.edu/dataset/887/national+health+and+nutrition+health+survey+2013-2014+(nhanes)+age+prediction+subset). It was originally prepared for a research paper on predicting diabetes and cardiovascular disease in patients (Dinh and Miertschin 2016). The dataset's stated purpose was to assess the health and nutritional status of adults and children in the United States, however respondents were classified as either Adults (respondents under 65 years of age) or Seniors (respondents 65 years of age or older). Respondents were located in the United States and provided data through interviews, physical examinations, and laboratory tests to the National Center for Health Statistics (NCHS) (part of the Centers for Disease Control and Prevention (CDC)).

The dataset has 10 variables and 2278 rows, with each row representing a respondent. The variables are:
1. SEQN - The respondent ID aka sequence number
2. age_group - The respondent's age group (adult or senior)
3. RIDAGEYR - The respondent's age in years
4. RIAGENDR - The respondent's gender (1 represents Male, 2 represents Female)
5. PAQ605 - Whether the respondent engages in weekly moderate or vigorous physical activity (1 means they do, 2 means they don't)
6. BMXBMI - The respondent's body mass index
7. LBXGLU - The respondent's blood glucose after fasting
8. DIQ010 - Whether the respondent is diabetic (1 is yes, 2 is no)
9. LBXGLT - A measure of the respondent's oral health
10. LBXIN - The respondent's blood insulin levels

According to the dataset description, there are no missing values, though EDA found some unexpected values for physical activity and diabetic. Since no explanation was provided for these codes, we omitted these respondent's from our analysis.

## Methods & Results

### Description of methods
We loaded and cleaned the data, first renaming columns for clarity. We then found values for physical activity and diabetic variables that were not explained in the dataset's documentation and decided to remove observations with those values. Next we confirmed the that the dataset's description of no missing values was accurate, then split the data into training and test, and conducted EDA on the training set - including examining summary statistics of each variable and plotting their distributions.

For our analysis, we first transformed categorical variables with one hot encoding, and standardized the scales of numeric variables. Because there were no missing values, it was not necessary to do transformations for this. We then fit 3 models (a dummy classifier, a logistic regression, and SVC) to the training data, and selected SVC for our final analysis because it had the best mean cross-validation accuracy. Finally, we used our SVC model to predict the test data and visualized how the model performed on this data.

### Imports & loading the data

#### Inspecting errors
The dataset source stated that "gender", "physical_activity", and "diabetic" are binary features. However, "physical_activity", "diabetic" contained three unique values instead of two. According to the dataset's documentation, 'physical_activity' should only have 1 or 2 as values so rows containing 7 should be omitted. Similarly, 'diabetic' should only have 1 or 2 as values so rows containing 3 should be omitted.

As a result, we removed 59 observations from the dataset during validation (1 case where physical activity was "7" and the remaining cases where diabetic was set to "3").

### Cleaning the data

#### Renaming columns and glancing at their values
We first renamed the columns of the data set to be more meaningful and easy to understand. Below is a short description of each column in the data set.

- RIDAGEYR: Respondent's Age
- RIAGENDR: Respondent's Gender (1 is Male / 2 is Female)
- PAQ605: Does the respondent engage in weekly moderate or vigorous-intensity physical activity (1 is yes / 2 is no)
- BMXBMI: Respondent's Body Mass Index
- LBXGLU: Respondent's Blood Glucose after fasting
- DIQ010: If the Respondent is diabetic (1 is yes / 2 is no)
- LBXGLT: Respondent's Oral
- LBXIN: Respondent's Blood Insulin Levels

### Splitting the data set

Prior to conducting EDA, we split the data set to avoid looking at the test data and influence the training of our model. The training data is 80% of the original dataset, and the test data is 20%.

### Conducting EDA on the training set

```{python}
# Checking the number of observations in the training data
X_train.info()
```

```{python}
# Generating summary statistics of our variables.
print("Table 5: Summary Statistics")
nhanes_summary = X_train.describe()
nhanes_summary
```

Since gender, physical_activity, and diabetic are categorical, only the mean and standard deviation from the table above are relevant for those columns. Body mass index values below 18 are considered underweight, and values over 40 are considered severely obese. We see that the middle 50% of values fall between 22.7 & 31.1, though the max is 70.1, which is concerningly high. Blood glucose, oral, and blood insulin have their own ranges, so it will likely be necessary to standardize these variables before fitting our model.

### Validating Training Data

```{python}
# Validation check: Target/response variable follows expected distribution

def validate_category_distribution(y_train, age_group_thresholds, tolerance):
    """
    Validate if a categorical variable's distribution meets specified thresholds with tolerance.

    Parameters:
    - y_train (pd.Series): The categorical variable (target/response variable).
    - age_group_thresholds (dict): Minimum and maximum proportion thresholds for each category.
    - tolerance (float): The tolerance to apply when checking proportions.

    Returns:
    - bool: True if the distribution meets the thresholds with tolerance, False otherwise.
    """
    value_counts = y_train.value_counts(normalize=True)  # Get proportions

    # Loop through each category and its thresholds
    for category, (min_threshold, max_threshold) in age_group_thresholds.items():
        proportion = value_counts.get(category, 0)  # Get proportion for the category
        
        # Check if the proportion is within the threshold range with tolerance
        if not (min_threshold - tolerance <= proportion <= max_threshold + tolerance):
            return False  # Return False if the proportion is out of the acceptable range
    
    return True  # Return True if all categories meet the criteria


age_group_thresholds = {"Adult": (0.2, 0.9), "Senior": (0.2, 0.9)}
tolerance = 0.05

# Validate the distribution
is_valid = validate_category_distribution(y_train, age_group_thresholds, tolerance)
print(is_valid)
```

```{python}
# validate training data for anomalous correlations between target/response variable 
# and features/explanatory variables, 
# as well as anomalous correlations between features/explanatory variables
 
train_df = pd.concat([X_train, y_train], axis = 1)

# Specify categorical features if applicable
categorical_features = ["gender", "physical_activity", "diabetic"]

# Initialize Deepchecks Dataset
train_df_ds = Dataset(train_df, label="age_group", cat_features=categorical_features)

# Feature-Label Correlation Check
check_feat_tar_corr = FeatureLabelCorrelation().add_condition_feature_pps_less_than(0.9)
check_feat_tar_corr_result = check_feat_tar_corr.run(dataset=train_df_ds)

# Feature-Feature Correlation Check
check_feat_feat_cor = FeatureFeatureCorrelation().add_condition_max_number_of_pairs_above_threshold(threshold=0.92, n_pairs=0)
check_feat_feat_cor_result = check_feat_feat_cor.run(dataset=train_df_ds)

# Validate conditions
if not check_feat_tar_corr_result.passed_conditions():
    raise ValueError("Feature-Label correlation exceeds the maximum acceptable threshold.")

if not check_feat_feat_cor_result.passed_conditions():
    raise ValueError("Feature-Feature correlation exceeds the maximum acceptable threshold.")

```

### Visualization for EDA

The distributions below show class imbalance, with very few seniors relative to adults in our dataset. Across numeric variables, mode values for seniors are less pronounced than they are for adult, though ranges seem similar. Seniors seem to have higher oral values and lower blood insulin values than adults. 

```{python}
# Plotting the distributions for each variable
alt.renderers.enable('png')
features_list = X_train.columns.tolist()

alt.Chart(pd.concat([X_train, y_train], axis = 1)).mark_bar(opacity = 1).encode(
            x=alt.X(alt.repeat()).type('quantitative').bin(maxbins=40).stack(False),
            y='count()',
            color = 'age_group'
        ).repeat(
            features_list,
            columns = 2
        ).properties(
            title="Fig 1: Feature Distributions by Age Group (groups are not stacked)"
        )
```

### Preprocessing features

#### Table 6: Feature types and planned transformations


| Feature | Transformation | Explanation
| --- | ----------- | ----- |
| gender | one-hot encoding with "binary=True" | A binary feature with no missing values. 1 is Male, 2 is Female.|
| physical_activity | one-hot encoding with "binary=True" | A binary feature with no missing values. 1 is Yes, 2 is No. |
| bmi | scaling with `StandardScaler` | A numeric feature with no missing values.  |
| blood_glucose | scaling with `StandardScaler`  | A numeric feature with no missing values. |
| diabetic | one-hot encoding with "binary=True"  | A binary feature with no missing values. 1 is Yes, 2 is No. |
| oral | scaling with `StandardScaler`  | A numeric feature with no missing values. |
| blood_insulin | scaling with `StandardScaler`  | A numeric feature with no missing values. |

#### Preprocessing

```{python}
# Transforming columns based on their type
numeric_features = ["bmi", "blood_glucose", "oral", "blood_insulin"]
binary_features = ["gender", "physical_activity", "diabetic"]

preprocessor = make_column_transformer(
    (OneHotEncoder(sparse_output = False,
                   drop='if_binary',dtype = int), binary_features),
    (StandardScaler(), numeric_features)
)

transformed_df = preprocessor.fit_transform(X_train)
```

### Comparing classification models on Train data for best model

We compare a dummy classifier, logistic regression, and SVC model by mean cross validation score.

#### Baseline model

```{python}
from sklearn.dummy import DummyClassifier
print("Table 7: DummyClassifier Cross validation results")

dummy = DummyClassifier(random_state = 123)
pipe = make_pipeline(preprocessor, dummy)
dc_results= cross_validate(
    pipe, X_train, y_train, cv=5, return_train_score=True
)
dc_results_df = pd.DataFrame(dc_results)
dc_results_df
```

#### Logistic regresion

```{python}
from sklearn.linear_model import LogisticRegression
print("Table 8: LogisticRegrssor Cross validation results")

lr = LogisticRegression(random_state = 123)
lr_pipe = make_pipeline(preprocessor, lr)
lr_results= cross_validate(
    lr_pipe, X_train, y_train, cv=5, return_train_score=True
)
lr_results_df = pd.DataFrame(lr_results)
lr_results_df
```

#### SVC model

```{python}
from sklearn.svm import SVC
print("Table 9: SVC Cross validation results")

svc = SVC(random_state = 123)
svc_pipe = make_pipeline(preprocessor, svc)
svc_results= cross_validate(
    svc_pipe, X_train, y_train, cv=5, return_train_score=True
)
svc_results_df = pd.DataFrame(svc_results)
svc_results_df
```

#### Comparing scores

```{python}
print("Model mean CV scores:")

# Baseline validation score
dc_test_score = dc_results_df['test_score'].mean()
print(f'  Baseline: {round(dc_test_score, 5)}')

# LR mean validation score
lr_mean_test_score = lr_results_df['test_score'].mean()
print(f'  Logistic Regression: {round(lr_mean_test_score, 5)}')

# SVC mean validation score
svc_mean_test_score = svc_results_df['test_score'].mean()
print(f'  SVC: {round(svc_mean_test_score, 5)}')

```

### Testing Best Model on Test Data

Since SVC had the best mean CV score, we selected it as our final model.

```{python}
svc_pipe.fit(X_train,y_train)

Test_score = svc_pipe.score(X_test,y_test)

print(f'Model accuracy on test data: {round(Test_score, 3)}')
```

#### Visualizing model performance

```{python}
from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay 
print("Fig 2: Confusion Matrix of best model on test data")

test_confMatrix = ConfusionMatrixDisplay.from_estimator(
    svc_pipe,
    X_test,
    y_test,
    values_format="d",
)
test_confMatrix
```

The confusion matrix shows that while the model score is 0.831, it does very poorly at recall (71 of 76 seniors are classified as adults) and quite poorly at precision (5/9 predicted seniors actually are seniors).

```{python}
print("Fig 3: ROC curve of best model on test data")

RocCurveDisplay.from_estimator(
    svc_pipe,
    X_test,
    y_test,
    pos_label= "Senior",
);
```

This performance is reflected in the ROC curve above. While it can differentiate the positive class "Senior" from the negative class to some extent, the model struggles to achieve both high true positive rates and low false positive rates.

## Discussion

The question we sought to answer was "Can information about the health and nutritional status of Americans be used to predict whether they are adults or seniors?" Our results indicate that yes, age group can be predicted with moderate accuracy (roughly 83%) based on health and nutritional inputs, however there is considerable room for model improvement. 

We were initially surprised how high accuracy was without any hyperparameter tuning, and this turned out to be because the classes were imbalanced, meaning accuracy as a metric oversells the model's ability to distinguish the two groups. Since adults are the majority class, classifying most respondents as adults will give a high accuracy, but not be useful for identifying seniors. 

In future research, we would use a metric like f1 to account for the class imbalance and conduct hyperparameter optimization to improve the model's recall and precision. 

One question for future research is to identify which health and nutritional factors have the strongest predictive ability for age group. Answering that could indicate which public health interventions have the most potential to balance health outcomes across age groups in America.

## References

Healthy Aging Center. 2022. "Aging Around the World." Colorado State University. https://www.research.colostate.edu/healthyagingcenter/2022/01/28/aging-around-the-world/.

Dinh, Andrew, and Susan Miertschin. 2016. “A Data-Driven Approach to Predicting Diabetes and Cardiovascular Disease with Machine Learning.” Semantic Scholar. https://www.semanticscholar.org/paper/A-data-driven-approach-to-predicting-diabetes-and-Dinh-Miertschin/01af1548ff1f3661d8bb813e8c35ee219a79ca9f.

Mukhtar, Hamid and Sana Al Azwari. “Investigating Non-Laboratory Variables to Predict Diabetic and Prediabetic Patients from Electronic Medical Records Using Machine Learning.” (2021).

Papazafiropoulou, Athanasia K.. “Diabetes management in the era of artificial intelligence.” Archives of Medical Sciences. Atherosclerotic Diseases 9 (2024): e122 - e128.

